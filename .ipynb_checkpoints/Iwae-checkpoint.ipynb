{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(\n",
    "        './data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms)\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "        './data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 64         # number of data points in each batch\n",
    "# N_EPOCHS = 10           # times to run the model on complete data\n",
    "# INPUT_DIM = 28 * 28     # size of each input\n",
    "# HIDDEN_DIM = 256        # hidden dimension\n",
    "# LATENT_DIM = 120         # latent vector dimension\n",
    "# N_CLASSES = 10          # number of classes in the data\n",
    "# lr = 1e-3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MNIST_Dataset(Dataset):    \n",
    "#     def __init__(self, image):\n",
    "#         super(MNIST_Dataset).__init__()\n",
    "#         self.image = image\n",
    "#     def __len__(self):\n",
    "#         return self.image.shape[0]\n",
    "#     def __getitem__(self, idx):\n",
    "#         return np.random.binomial(1, self.image[idx, :]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.transform = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                       nn.Tanh(),\n",
    "                                       nn.Linear(hidden_dim, hidden_dim),\n",
    "                                       nn.Tanh())\n",
    "        self.fc_mu = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc_logsigma = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.transform(x)\n",
    "        mu = self.fc_mu(out)\n",
    "        logsigma = self.fc_logsigma(out)\n",
    "        sigma = torch.exp(logsigma)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IWAE_1(nn.Module):\n",
    "    def __init__(self, dim_h1, dim_image_vars):\n",
    "        super(IWAE_1, self).__init__()\n",
    "        self.dim_h1 = dim_h1\n",
    "        self.dim_image_vars = dim_image_vars\n",
    "\n",
    "        ## encoder\n",
    "        self.encoder_h1 = BasicBlock(dim_image_vars, 200, dim_h1)\n",
    "        \n",
    "        ## decoder\n",
    "        self.decoder_x =  nn.Sequential(nn.Linear(dim_h1, 200),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Linear(200, 200),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Linear(200, dim_image_vars),\n",
    "                                        nn.Sigmoid())\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        mu_h1, sigma_h1 = self.encoder_h1(x)\n",
    "        eps = Variable(sigma_h1.data.new(sigma_h1.size()).normal_())\n",
    "        h1 = mu_h1 + sigma_h1 * eps   \n",
    "        return h1, mu_h1, sigma_h1, eps\n",
    "    \n",
    "    def decoder(self, h1):\n",
    "        p = self.decoder_x(h1)\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1, mu_h1, sigma_h1, eps = self.encoder(x)\n",
    "        p = self.decoder(h1)\n",
    "        return (h1, mu_h1, sigma_h1, eps), (p)\n",
    "\n",
    "    def train_loss(self, inputs):\n",
    "        h1, mu_h1, sigma_h1, eps = self.encoder(inputs)\n",
    "        #log_Qh1Gx = torch.sum(-0.5*((h1-mu_h1)/sigma_h1)**2 - torch.log(sigma_h1), -1)\n",
    "        log_Qh1Gx = torch.sum(-0.5*(eps)**2 - torch.log(sigma_h1), -1)\n",
    "        \n",
    "        p = self.decoder(h1)\n",
    "        log_Ph1 = torch.sum(-0.5*h1**2, -1)\n",
    "        log_PxGh1 = torch.sum(inputs*torch.log(p) + (1-inputs)*torch.log(1-p), -1)\n",
    "\n",
    "        log_weight = log_Ph1 + log_PxGh1 - log_Qh1Gx\n",
    "        log_weight = log_weight - torch.max(log_weight, 0)[0]\n",
    "        weight = torch.exp(log_weight)\n",
    "        weight = weight / torch.sum(weight, 0)\n",
    "        weight = Variable(weight.data, requires_grad = False)\n",
    "        loss = -torch.mean(torch.sum(weight * (log_Ph1 + log_PxGh1 - log_Qh1Gx), 0))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_loss(self, inputs):\n",
    "        h1, mu_h1, sigma_h1, eps = self.encoder(inputs)\n",
    "        #log_Qh1Gx = torch.sum(-0.5*((h1-mu_h1)/sigma_h1)**2 - torch.log(sigma_h1), -1)\n",
    "        log_Qh1Gx = torch.sum(-0.5*(eps)**2 - torch.log(sigma_h1), -1)\n",
    "        \n",
    "        p = self.decoder(h1)\n",
    "        log_Ph1 = torch.sum(-0.5*h1**2, -1)\n",
    "        log_PxGh1 = torch.sum(inputs*torch.log(p) + (1-inputs)*torch.log(1-p), -1)\n",
    "\n",
    "        log_weight = log_Ph1 + log_PxGh1 - log_Qh1Gx\n",
    "        weight = torch.exp(log_weight)\n",
    "        loss = -torch.mean(torch.log(torch.mean(weight, 0)))        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = IWAE_1(50, 784)\n",
    "#vae.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0, Step:    0, loss: 544.42\n",
      "Epoch:    0, Step:    1, loss: 538.58\n",
      "Epoch:    0, Step:    2, loss: 529.77\n",
      "Epoch:    0, Step:    3, loss: 515.10\n",
      "Epoch:    0, Step:    4, loss: 493.06\n",
      "Epoch:    0, Step:    5, loss: 463.95\n",
      "Epoch:    0, Step:    6, loss: 431.69\n",
      "Epoch:    0, Step:    7, loss: 398.88\n",
      "Epoch:    0, Step:    8, loss: 370.07\n",
      "Epoch:    0, Step:    9, loss: 344.09\n",
      "Epoch:    0, Step:   10, loss: 321.41\n",
      "Epoch:    0, Step:   11, loss: 302.29\n",
      "Epoch:    0, Step:   12, loss: 284.49\n",
      "Epoch:    0, Step:   13, loss: 271.15\n",
      "Epoch:    0, Step:   14, loss: 258.29\n",
      "Epoch:    0, Step:   15, loss: 250.90\n",
      "Epoch:    0, Step:   16, loss: 242.57\n",
      "Epoch:    0, Step:   17, loss: 238.01\n",
      "Epoch:    0, Step:   18, loss: 231.75\n",
      "Epoch:    0, Step:   19, loss: 229.80\n",
      "Epoch:    0, Step:   20, loss: 223.54\n",
      "Epoch:    0, Step:   21, loss: 222.48\n",
      "Epoch:    0, Step:   22, loss: 218.16\n",
      "Epoch:    0, Step:   23, loss: 219.98\n",
      "Epoch:    0, Step:   24, loss: 215.35\n",
      "Epoch:    0, Step:   25, loss: 213.66\n",
      "Epoch:    0, Step:   26, loss: 215.50\n",
      "Epoch:    0, Step:   27, loss: 211.64\n",
      "Epoch:    0, Step:   28, loss: 213.48\n",
      "Epoch:    0, Step:   29, loss: 212.76\n",
      "Epoch:    0, Step:   30, loss: 211.10\n",
      "Epoch:    0, Step:   31, loss: 209.90\n",
      "Epoch:    0, Step:   32, loss: 207.52\n",
      "Epoch:    0, Step:   33, loss: 210.17\n",
      "Epoch:    0, Step:   34, loss: 210.25\n",
      "Epoch:    0, Step:   35, loss: 209.13\n",
      "Epoch:    0, Step:   36, loss: 208.63\n",
      "Epoch:    0, Step:   37, loss: 207.15\n",
      "Epoch:    0, Step:   38, loss: 206.98\n",
      "Epoch:    0, Step:   39, loss: 206.61\n",
      "Epoch:    0, Step:   40, loss: 207.05\n",
      "Epoch:    0, Step:   41, loss: 209.16\n",
      "Epoch:    0, Step:   42, loss: 205.64\n",
      "Epoch:    0, Step:   43, loss: 207.66\n",
      "Epoch:    0, Step:   44, loss: 206.41\n",
      "Epoch:    0, Step:   45, loss: 206.80\n",
      "Epoch:    0, Step:   46, loss: 204.64\n",
      "Epoch:    0, Step:   47, loss: 206.32\n",
      "Epoch:    0, Step:   48, loss: 206.19\n",
      "Epoch:    0, Step:   49, loss: 205.46\n",
      "Epoch:    0, Step:   50, loss: 208.32\n",
      "Epoch:    0, Step:   51, loss: 203.95\n",
      "Epoch:    0, Step:   52, loss: 208.03\n",
      "Epoch:    0, Step:   53, loss: 206.34\n",
      "Epoch:    0, Step:   54, loss: 204.45\n",
      "Epoch:    0, Step:   55, loss: 204.61\n",
      "Epoch:    0, Step:   56, loss: 202.84\n",
      "Epoch:    0, Step:   57, loss: 206.12\n",
      "Epoch:    0, Step:   58, loss: 203.47\n",
      "Epoch:    0, Step:   59, loss: 204.77\n",
      "Epoch:    1, Step:    0, loss: 204.02\n",
      "Epoch:    1, Step:    1, loss: 203.16\n",
      "Epoch:    1, Step:    2, loss: 205.75\n",
      "Epoch:    1, Step:    3, loss: 205.57\n",
      "Epoch:    1, Step:    4, loss: 204.14\n",
      "Epoch:    1, Step:    5, loss: 203.10\n",
      "Epoch:    1, Step:    6, loss: 204.03\n",
      "Epoch:    1, Step:    7, loss: 205.58\n",
      "Epoch:    1, Step:    8, loss: 206.33\n",
      "Epoch:    1, Step:    9, loss: 206.18\n",
      "Epoch:    1, Step:   10, loss: 205.81\n",
      "Epoch:    1, Step:   11, loss: 204.53\n",
      "Epoch:    1, Step:   12, loss: 207.88\n",
      "Epoch:    1, Step:   13, loss: 205.01\n",
      "Epoch:    1, Step:   14, loss: 204.49\n",
      "Epoch:    1, Step:   15, loss: 201.61\n",
      "Epoch:    1, Step:   16, loss: 206.53\n",
      "Epoch:    1, Step:   17, loss: 206.00\n",
      "Epoch:    1, Step:   18, loss: 204.70\n",
      "Epoch:    1, Step:   19, loss: 201.03\n",
      "Epoch:    1, Step:   20, loss: 204.21\n",
      "Epoch:    1, Step:   21, loss: 203.99\n",
      "Epoch:    1, Step:   22, loss: 201.13\n",
      "Epoch:    1, Step:   23, loss: 202.93\n",
      "Epoch:    1, Step:   24, loss: 202.97\n",
      "Epoch:    1, Step:   25, loss: 205.04\n",
      "Epoch:    1, Step:   26, loss: 204.03\n",
      "Epoch:    1, Step:   27, loss: 205.26\n",
      "Epoch:    1, Step:   28, loss: 207.51\n",
      "Epoch:    1, Step:   29, loss: 203.77\n",
      "Epoch:    1, Step:   30, loss: 203.86\n",
      "Epoch:    1, Step:   31, loss: 202.30\n",
      "Epoch:    1, Step:   32, loss: 205.24\n",
      "Epoch:    1, Step:   33, loss: 201.52\n",
      "Epoch:    1, Step:   34, loss: 202.06\n",
      "Epoch:    1, Step:   35, loss: 199.68\n",
      "Epoch:    1, Step:   36, loss: 202.89\n",
      "Epoch:    1, Step:   37, loss: 203.90\n",
      "Epoch:    1, Step:   38, loss: 205.05\n",
      "Epoch:    1, Step:   39, loss: 201.59\n",
      "Epoch:    1, Step:   40, loss: 203.11\n",
      "Epoch:    1, Step:   41, loss: 202.61\n",
      "Epoch:    1, Step:   42, loss: 205.44\n",
      "Epoch:    1, Step:   43, loss: 204.71\n",
      "Epoch:    1, Step:   44, loss: 201.52\n",
      "Epoch:    1, Step:   45, loss: 201.92\n",
      "Epoch:    1, Step:   46, loss: 201.06\n",
      "Epoch:    1, Step:   47, loss: 202.00\n",
      "Epoch:    1, Step:   48, loss: 202.49\n",
      "Epoch:    1, Step:   49, loss: 201.12\n",
      "Epoch:    1, Step:   50, loss: 201.96\n",
      "Epoch:    1, Step:   51, loss: 199.59\n",
      "Epoch:    1, Step:   52, loss: 201.61\n",
      "Epoch:    1, Step:   53, loss: 201.88\n",
      "Epoch:    1, Step:   54, loss: 200.88\n",
      "Epoch:    1, Step:   55, loss: 199.89\n",
      "Epoch:    1, Step:   56, loss: 200.06\n",
      "Epoch:    1, Step:   57, loss: 199.18\n",
      "Epoch:    1, Step:   58, loss: 198.11\n",
      "Epoch:    1, Step:   59, loss: 200.12\n",
      "Epoch:    2, Step:    0, loss: 199.82\n",
      "Epoch:    2, Step:    1, loss: 199.40\n",
      "Epoch:    2, Step:    2, loss: 200.58\n",
      "Epoch:    2, Step:    3, loss: 197.49\n",
      "Epoch:    2, Step:    4, loss: 198.27\n",
      "Epoch:    2, Step:    5, loss: 198.32\n",
      "Epoch:    2, Step:    6, loss: 198.78\n",
      "Epoch:    2, Step:    7, loss: 200.55\n",
      "Epoch:    2, Step:    8, loss: 198.95\n",
      "Epoch:    2, Step:    9, loss: 198.59\n",
      "Epoch:    2, Step:   10, loss: 198.09\n",
      "Epoch:    2, Step:   11, loss: 196.46\n",
      "Epoch:    2, Step:   12, loss: 196.50\n",
      "Epoch:    2, Step:   13, loss: 196.83\n",
      "Epoch:    2, Step:   14, loss: 194.53\n",
      "Epoch:    2, Step:   15, loss: 193.97\n",
      "Epoch:    2, Step:   16, loss: 195.61\n",
      "Epoch:    2, Step:   17, loss: 195.19\n",
      "Epoch:    2, Step:   18, loss: 197.63\n",
      "Epoch:    2, Step:   19, loss: 196.02\n",
      "Epoch:    2, Step:   20, loss: 194.40\n",
      "Epoch:    2, Step:   21, loss: 194.68\n",
      "Epoch:    2, Step:   22, loss: 197.08\n",
      "Epoch:    2, Step:   23, loss: 193.85\n",
      "Epoch:    2, Step:   24, loss: 196.29\n",
      "Epoch:    2, Step:   25, loss: 196.77\n",
      "Epoch:    2, Step:   26, loss: 193.88\n",
      "Epoch:    2, Step:   27, loss: 194.83\n",
      "Epoch:    2, Step:   28, loss: 193.79\n",
      "Epoch:    2, Step:   29, loss: 197.07\n",
      "Epoch:    2, Step:   30, loss: 193.39\n",
      "Epoch:    2, Step:   31, loss: 194.20\n",
      "Epoch:    2, Step:   32, loss: 194.23\n",
      "Epoch:    2, Step:   33, loss: 194.66\n",
      "Epoch:    2, Step:   34, loss: 193.62\n",
      "Epoch:    2, Step:   35, loss: 193.67\n",
      "Epoch:    2, Step:   36, loss: 193.75\n",
      "Epoch:    2, Step:   37, loss: 196.07\n",
      "Epoch:    2, Step:   38, loss: 193.06\n",
      "Epoch:    2, Step:   39, loss: 196.47\n",
      "Epoch:    2, Step:   40, loss: 194.24\n",
      "Epoch:    2, Step:   41, loss: 195.61\n",
      "Epoch:    2, Step:   42, loss: 193.27\n",
      "Epoch:    2, Step:   43, loss: 192.85\n",
      "Epoch:    2, Step:   44, loss: 194.96\n",
      "Epoch:    2, Step:   45, loss: 195.24\n",
      "Epoch:    2, Step:   46, loss: 192.15\n",
      "Epoch:    2, Step:   47, loss: 195.21\n",
      "Epoch:    2, Step:   48, loss: 191.62\n",
      "Epoch:    2, Step:   49, loss: 193.05\n",
      "Epoch:    2, Step:   50, loss: 191.06\n",
      "Epoch:    2, Step:   51, loss: 192.26\n",
      "Epoch:    2, Step:   52, loss: 191.05\n",
      "Epoch:    2, Step:   53, loss: 193.88\n",
      "Epoch:    2, Step:   54, loss: 195.47\n",
      "Epoch:    2, Step:   55, loss: 193.03\n",
      "Epoch:    2, Step:   56, loss: 192.58\n",
      "Epoch:    2, Step:   57, loss: 192.82\n",
      "Epoch:    2, Step:   58, loss: 195.49\n",
      "Epoch:    2, Step:   59, loss: 193.61\n",
      "Epoch:    3, Step:    0, loss: 192.35\n",
      "Epoch:    3, Step:    1, loss: 193.18\n",
      "Epoch:    3, Step:    2, loss: 191.91\n",
      "Epoch:    3, Step:    3, loss: 190.25\n",
      "Epoch:    3, Step:    4, loss: 192.86\n",
      "Epoch:    3, Step:    5, loss: 190.90\n",
      "Epoch:    3, Step:    6, loss: 191.53\n",
      "Epoch:    3, Step:    7, loss: 192.64\n",
      "Epoch:    3, Step:    8, loss: 192.93\n",
      "Epoch:    3, Step:    9, loss: 191.12\n",
      "Epoch:    3, Step:   10, loss: 194.00\n",
      "Epoch:    3, Step:   11, loss: 190.98\n",
      "Epoch:    3, Step:   12, loss: 193.43\n",
      "Epoch:    3, Step:   13, loss: 190.60\n",
      "Epoch:    3, Step:   14, loss: 194.75\n",
      "Epoch:    3, Step:   15, loss: 191.07\n",
      "Epoch:    3, Step:   16, loss: 192.72\n",
      "Epoch:    3, Step:   17, loss: 188.26\n",
      "Epoch:    3, Step:   18, loss: 192.35\n",
      "Epoch:    3, Step:   19, loss: 190.40\n",
      "Epoch:    3, Step:   20, loss: 189.59\n",
      "Epoch:    3, Step:   21, loss: 194.53\n",
      "Epoch:    3, Step:   22, loss: 191.33\n",
      "Epoch:    3, Step:   23, loss: 191.68\n",
      "Epoch:    3, Step:   24, loss: 192.26\n",
      "Epoch:    3, Step:   25, loss: 191.45\n",
      "Epoch:    3, Step:   26, loss: 191.13\n",
      "Epoch:    3, Step:   27, loss: 190.72\n",
      "Epoch:    3, Step:   28, loss: 192.07\n",
      "Epoch:    3, Step:   29, loss: 191.18\n",
      "Epoch:    3, Step:   30, loss: 194.37\n",
      "Epoch:    3, Step:   31, loss: 191.39\n",
      "Epoch:    3, Step:   32, loss: 192.24\n",
      "Epoch:    3, Step:   33, loss: 191.08\n",
      "Epoch:    3, Step:   34, loss: 190.67\n",
      "Epoch:    3, Step:   35, loss: 192.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    3, Step:   36, loss: 192.57\n",
      "Epoch:    3, Step:   37, loss: 190.40\n",
      "Epoch:    3, Step:   38, loss: 191.00\n",
      "Epoch:    3, Step:   39, loss: 189.79\n",
      "Epoch:    3, Step:   40, loss: 191.54\n",
      "Epoch:    3, Step:   41, loss: 190.30\n",
      "Epoch:    3, Step:   42, loss: 188.73\n",
      "Epoch:    3, Step:   43, loss: 190.72\n",
      "Epoch:    3, Step:   44, loss: 190.76\n",
      "Epoch:    3, Step:   45, loss: 189.66\n",
      "Epoch:    3, Step:   46, loss: 190.26\n",
      "Epoch:    3, Step:   47, loss: 191.79\n",
      "Epoch:    3, Step:   48, loss: 190.31\n",
      "Epoch:    3, Step:   49, loss: 188.36\n",
      "Epoch:    3, Step:   50, loss: 189.26\n",
      "Epoch:    3, Step:   51, loss: 189.42\n",
      "Epoch:    3, Step:   52, loss: 192.99\n",
      "Epoch:    3, Step:   53, loss: 189.28\n",
      "Epoch:    3, Step:   54, loss: 190.27\n",
      "Epoch:    3, Step:   55, loss: 191.87\n",
      "Epoch:    3, Step:   56, loss: 188.95\n",
      "Epoch:    3, Step:   57, loss: 190.48\n",
      "Epoch:    3, Step:   58, loss: 187.72\n",
      "Epoch:    3, Step:   59, loss: 189.56\n",
      "Epoch:    4, Step:    0, loss: 189.96\n",
      "Epoch:    4, Step:    1, loss: 189.01\n",
      "Epoch:    4, Step:    2, loss: 189.96\n",
      "Epoch:    4, Step:    3, loss: 187.78\n",
      "Epoch:    4, Step:    4, loss: 187.81\n",
      "Epoch:    4, Step:    5, loss: 186.67\n",
      "Epoch:    4, Step:    6, loss: 186.29\n",
      "Epoch:    4, Step:    7, loss: 186.98\n",
      "Epoch:    4, Step:    8, loss: 186.67\n",
      "Epoch:    4, Step:    9, loss: 184.90\n",
      "Epoch:    4, Step:   10, loss: 185.97\n",
      "Epoch:    4, Step:   11, loss: 184.63\n",
      "Epoch:    4, Step:   12, loss: 187.68\n",
      "Epoch:    4, Step:   13, loss: 186.99\n",
      "Epoch:    4, Step:   14, loss: 187.69\n",
      "Epoch:    4, Step:   15, loss: 184.83\n",
      "Epoch:    4, Step:   16, loss: 185.08\n",
      "Epoch:    4, Step:   17, loss: 185.99\n",
      "Epoch:    4, Step:   18, loss: 182.17\n",
      "Epoch:    4, Step:   19, loss: 184.95\n",
      "Epoch:    4, Step:   20, loss: 186.80\n",
      "Epoch:    4, Step:   21, loss: 183.04\n",
      "Epoch:    4, Step:   22, loss: 181.48\n",
      "Epoch:    4, Step:   23, loss: 183.88\n",
      "Epoch:    4, Step:   24, loss: 182.33\n",
      "Epoch:    4, Step:   25, loss: 182.25\n",
      "Epoch:    4, Step:   26, loss: 182.86\n",
      "Epoch:    4, Step:   27, loss: 183.84\n",
      "Epoch:    4, Step:   28, loss: 182.22\n",
      "Epoch:    4, Step:   29, loss: 181.82\n",
      "Epoch:    4, Step:   30, loss: 182.03\n",
      "Epoch:    4, Step:   31, loss: 181.46\n",
      "Epoch:    4, Step:   32, loss: 180.69\n",
      "Epoch:    4, Step:   33, loss: 181.37\n",
      "Epoch:    4, Step:   34, loss: 180.93\n",
      "Epoch:    4, Step:   35, loss: 183.49\n",
      "Epoch:    4, Step:   36, loss: 180.13\n",
      "Epoch:    4, Step:   37, loss: 178.99\n",
      "Epoch:    4, Step:   38, loss: 181.60\n",
      "Epoch:    4, Step:   39, loss: 179.94\n",
      "Epoch:    4, Step:   40, loss: 178.47\n",
      "Epoch:    4, Step:   41, loss: 180.19\n",
      "Epoch:    4, Step:   42, loss: 181.81\n",
      "Epoch:    4, Step:   43, loss: 180.05\n",
      "Epoch:    4, Step:   44, loss: 177.99\n",
      "Epoch:    4, Step:   45, loss: 179.39\n",
      "Epoch:    4, Step:   46, loss: 180.28\n",
      "Epoch:    4, Step:   47, loss: 177.65\n",
      "Epoch:    4, Step:   48, loss: 177.30\n",
      "Epoch:    4, Step:   49, loss: 180.36\n",
      "Epoch:    4, Step:   50, loss: 180.03\n",
      "Epoch:    4, Step:   51, loss: 179.17\n",
      "Epoch:    4, Step:   52, loss: 175.69\n",
      "Epoch:    4, Step:   53, loss: 175.39\n",
      "Epoch:    4, Step:   54, loss: 178.84\n",
      "Epoch:    4, Step:   55, loss: 175.21\n",
      "Epoch:    4, Step:   56, loss: 177.24\n",
      "Epoch:    4, Step:   57, loss: 177.78\n",
      "Epoch:    4, Step:   58, loss: 177.89\n",
      "Epoch:    4, Step:   59, loss: 179.95\n",
      "Epoch:    5, Step:    0, loss: 176.18\n",
      "Epoch:    5, Step:    1, loss: 174.61\n",
      "Epoch:    5, Step:    2, loss: 173.97\n",
      "Epoch:    5, Step:    3, loss: 174.93\n",
      "Epoch:    5, Step:    4, loss: 174.01\n",
      "Epoch:    5, Step:    5, loss: 174.33\n",
      "Epoch:    5, Step:    6, loss: 176.60\n",
      "Epoch:    5, Step:    7, loss: 173.56\n",
      "Epoch:    5, Step:    8, loss: 173.88\n",
      "Epoch:    5, Step:    9, loss: 171.97\n",
      "Epoch:    5, Step:   10, loss: 174.85\n",
      "Epoch:    5, Step:   11, loss: 172.34\n",
      "Epoch:    5, Step:   12, loss: 171.14\n",
      "Epoch:    5, Step:   13, loss: 171.57\n",
      "Epoch:    5, Step:   14, loss: 173.52\n",
      "Epoch:    5, Step:   15, loss: 172.85\n",
      "Epoch:    5, Step:   16, loss: 173.99\n",
      "Epoch:    5, Step:   17, loss: 171.76\n",
      "Epoch:    5, Step:   18, loss: 171.58\n",
      "Epoch:    5, Step:   19, loss: 172.35\n",
      "Epoch:    5, Step:   20, loss: 171.29\n",
      "Epoch:    5, Step:   21, loss: 170.34\n",
      "Epoch:    5, Step:   22, loss: 169.61\n",
      "Epoch:    5, Step:   23, loss: 172.30\n",
      "Epoch:    5, Step:   24, loss: 171.73\n",
      "Epoch:    5, Step:   25, loss: 171.60\n",
      "Epoch:    5, Step:   26, loss: 171.10\n",
      "Epoch:    5, Step:   27, loss: 169.33\n",
      "Epoch:    5, Step:   28, loss: 173.64\n",
      "Epoch:    5, Step:   29, loss: 172.21\n",
      "Epoch:    5, Step:   30, loss: 169.59\n",
      "Epoch:    5, Step:   31, loss: 168.78\n",
      "Epoch:    5, Step:   32, loss: 172.23\n",
      "Epoch:    5, Step:   33, loss: 171.27\n",
      "Epoch:    5, Step:   34, loss: 171.77\n",
      "Epoch:    5, Step:   35, loss: 170.39\n",
      "Epoch:    5, Step:   36, loss: 170.86\n",
      "Epoch:    5, Step:   37, loss: 169.53\n",
      "Epoch:    5, Step:   38, loss: 169.24\n",
      "Epoch:    5, Step:   39, loss: 168.92\n",
      "Epoch:    5, Step:   40, loss: 167.84\n",
      "Epoch:    5, Step:   41, loss: 170.10\n",
      "Epoch:    5, Step:   42, loss: 169.24\n",
      "Epoch:    5, Step:   43, loss: 168.09\n",
      "Epoch:    5, Step:   44, loss: 170.04\n",
      "Epoch:    5, Step:   45, loss: 168.05\n",
      "Epoch:    5, Step:   46, loss: 168.87\n",
      "Epoch:    5, Step:   47, loss: 171.81\n",
      "Epoch:    5, Step:   48, loss: 171.31\n",
      "Epoch:    5, Step:   49, loss: 171.95\n",
      "Epoch:    5, Step:   50, loss: 170.01\n",
      "Epoch:    5, Step:   51, loss: 169.87\n",
      "Epoch:    5, Step:   52, loss: 169.57\n",
      "Epoch:    5, Step:   53, loss: 168.56\n",
      "Epoch:    5, Step:   54, loss: 169.87\n",
      "Epoch:    5, Step:   55, loss: 168.73\n",
      "Epoch:    5, Step:   56, loss: 165.85\n",
      "Epoch:    5, Step:   57, loss: 167.07\n",
      "Epoch:    5, Step:   58, loss: 167.63\n",
      "Epoch:    5, Step:   59, loss: 169.58\n",
      "Epoch:    6, Step:    0, loss: 166.90\n",
      "Epoch:    6, Step:    1, loss: 165.69\n",
      "Epoch:    6, Step:    2, loss: 167.42\n",
      "Epoch:    6, Step:    3, loss: 167.67\n",
      "Epoch:    6, Step:    4, loss: 166.34\n",
      "Epoch:    6, Step:    5, loss: 164.92\n",
      "Epoch:    6, Step:    6, loss: 166.70\n",
      "Epoch:    6, Step:    7, loss: 165.66\n",
      "Epoch:    6, Step:    8, loss: 166.38\n",
      "Epoch:    6, Step:    9, loss: 166.31\n",
      "Epoch:    6, Step:   10, loss: 167.86\n",
      "Epoch:    6, Step:   11, loss: 165.83\n",
      "Epoch:    6, Step:   12, loss: 163.17\n",
      "Epoch:    6, Step:   13, loss: 164.00\n",
      "Epoch:    6, Step:   14, loss: 165.34\n",
      "Epoch:    6, Step:   15, loss: 166.12\n",
      "Epoch:    6, Step:   16, loss: 165.68\n",
      "Epoch:    6, Step:   17, loss: 164.41\n",
      "Epoch:    6, Step:   18, loss: 163.02\n",
      "Epoch:    6, Step:   19, loss: 166.01\n",
      "Epoch:    6, Step:   20, loss: 165.16\n",
      "Epoch:    6, Step:   21, loss: 163.24\n",
      "Epoch:    6, Step:   22, loss: 162.42\n",
      "Epoch:    6, Step:   23, loss: 163.48\n",
      "Epoch:    6, Step:   24, loss: 163.95\n",
      "Epoch:    6, Step:   25, loss: 167.06\n",
      "Epoch:    6, Step:   26, loss: 161.35\n",
      "Epoch:    6, Step:   27, loss: 161.92\n",
      "Epoch:    6, Step:   28, loss: 163.69\n",
      "Epoch:    6, Step:   29, loss: 161.98\n",
      "Epoch:    6, Step:   30, loss: 164.84\n",
      "Epoch:    6, Step:   31, loss: 163.95\n",
      "Epoch:    6, Step:   32, loss: 162.51\n",
      "Epoch:    6, Step:   33, loss: 163.63\n",
      "Epoch:    6, Step:   34, loss: 163.39\n",
      "Epoch:    6, Step:   35, loss: 159.38\n",
      "Epoch:    6, Step:   36, loss: 163.49\n",
      "Epoch:    6, Step:   37, loss: 162.17\n",
      "Epoch:    6, Step:   38, loss: 161.56\n",
      "Epoch:    6, Step:   39, loss: 162.33\n",
      "Epoch:    6, Step:   40, loss: 160.90\n",
      "Epoch:    6, Step:   41, loss: 159.24\n",
      "Epoch:    6, Step:   42, loss: 159.49\n",
      "Epoch:    6, Step:   43, loss: 162.04\n",
      "Epoch:    6, Step:   44, loss: 162.42\n",
      "Epoch:    6, Step:   45, loss: 160.42\n",
      "Epoch:    6, Step:   46, loss: 159.06\n",
      "Epoch:    6, Step:   47, loss: 158.64\n",
      "Epoch:    6, Step:   48, loss: 160.15\n",
      "Epoch:    6, Step:   49, loss: 161.65\n",
      "Epoch:    6, Step:   50, loss: 161.34\n",
      "Epoch:    6, Step:   51, loss: 159.65\n",
      "Epoch:    6, Step:   52, loss: 159.40\n",
      "Epoch:    6, Step:   53, loss: 159.24\n",
      "Epoch:    6, Step:   54, loss: 159.54\n",
      "Epoch:    6, Step:   55, loss: 158.71\n",
      "Epoch:    6, Step:   56, loss: 160.39\n",
      "Epoch:    6, Step:   57, loss: 157.24\n",
      "Epoch:    6, Step:   58, loss: 156.73\n",
      "Epoch:    6, Step:   59, loss: 155.46\n",
      "Epoch:    7, Step:    0, loss: 159.94\n",
      "Epoch:    7, Step:    1, loss: 156.77\n",
      "Epoch:    7, Step:    2, loss: 158.65\n",
      "Epoch:    7, Step:    3, loss: 157.32\n",
      "Epoch:    7, Step:    4, loss: 158.67\n",
      "Epoch:    7, Step:    5, loss: 158.04\n",
      "Epoch:    7, Step:    6, loss: 157.39\n",
      "Epoch:    7, Step:    7, loss: 158.56\n",
      "Epoch:    7, Step:    8, loss: 159.28\n",
      "Epoch:    7, Step:    9, loss: 158.71\n",
      "Epoch:    7, Step:   10, loss: 158.76\n",
      "Epoch:    7, Step:   11, loss: 156.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    7, Step:   12, loss: 156.24\n",
      "Epoch:    7, Step:   13, loss: 158.68\n",
      "Epoch:    7, Step:   14, loss: 157.31\n",
      "Epoch:    7, Step:   15, loss: 156.29\n",
      "Epoch:    7, Step:   16, loss: 155.96\n",
      "Epoch:    7, Step:   17, loss: 157.21\n",
      "Epoch:    7, Step:   18, loss: 155.76\n",
      "Epoch:    7, Step:   19, loss: 155.29\n",
      "Epoch:    7, Step:   20, loss: 156.46\n",
      "Epoch:    7, Step:   21, loss: 155.40\n",
      "Epoch:    7, Step:   22, loss: 153.12\n",
      "Epoch:    7, Step:   23, loss: 155.84\n",
      "Epoch:    7, Step:   24, loss: 156.82\n",
      "Epoch:    7, Step:   25, loss: 155.60\n",
      "Epoch:    7, Step:   26, loss: 156.66\n",
      "Epoch:    7, Step:   27, loss: 156.27\n",
      "Epoch:    7, Step:   28, loss: 156.05\n",
      "Epoch:    7, Step:   29, loss: 152.72\n",
      "Epoch:    7, Step:   30, loss: 154.03\n",
      "Epoch:    7, Step:   31, loss: 153.97\n",
      "Epoch:    7, Step:   32, loss: 154.80\n",
      "Epoch:    7, Step:   33, loss: 154.44\n",
      "Epoch:    7, Step:   34, loss: 154.89\n",
      "Epoch:    7, Step:   35, loss: 153.87\n",
      "Epoch:    7, Step:   36, loss: 155.69\n",
      "Epoch:    7, Step:   37, loss: 152.56\n",
      "Epoch:    7, Step:   38, loss: 153.77\n",
      "Epoch:    7, Step:   39, loss: 154.17\n",
      "Epoch:    7, Step:   40, loss: 154.87\n",
      "Epoch:    7, Step:   41, loss: 153.77\n",
      "Epoch:    7, Step:   42, loss: 152.87\n",
      "Epoch:    7, Step:   43, loss: 153.56\n",
      "Epoch:    7, Step:   44, loss: 154.28\n",
      "Epoch:    7, Step:   45, loss: 153.89\n",
      "Epoch:    7, Step:   46, loss: 153.29\n",
      "Epoch:    7, Step:   47, loss: 154.59\n",
      "Epoch:    7, Step:   48, loss: 153.41\n",
      "Epoch:    7, Step:   49, loss: 155.47\n",
      "Epoch:    7, Step:   50, loss: 152.51\n",
      "Epoch:    7, Step:   51, loss: 150.94\n",
      "Epoch:    7, Step:   52, loss: 151.80\n",
      "Epoch:    7, Step:   53, loss: 152.85\n",
      "Epoch:    7, Step:   54, loss: 149.79\n",
      "Epoch:    7, Step:   55, loss: 152.94\n",
      "Epoch:    7, Step:   56, loss: 149.43\n",
      "Epoch:    7, Step:   57, loss: 151.46\n",
      "Epoch:    7, Step:   58, loss: 151.57\n",
      "Epoch:    7, Step:   59, loss: 150.78\n",
      "Epoch:    8, Step:    0, loss: 152.23\n",
      "Epoch:    8, Step:    1, loss: 150.69\n",
      "Epoch:    8, Step:    2, loss: 152.28\n",
      "Epoch:    8, Step:    3, loss: 151.41\n",
      "Epoch:    8, Step:    4, loss: 152.62\n",
      "Epoch:    8, Step:    5, loss: 149.84\n",
      "Epoch:    8, Step:    6, loss: 149.71\n",
      "Epoch:    8, Step:    7, loss: 151.57\n",
      "Epoch:    8, Step:    8, loss: 153.48\n",
      "Epoch:    8, Step:    9, loss: 152.10\n",
      "Epoch:    8, Step:   10, loss: 148.51\n",
      "Epoch:    8, Step:   11, loss: 152.27\n",
      "Epoch:    8, Step:   12, loss: 151.15\n",
      "Epoch:    8, Step:   13, loss: 152.47\n",
      "Epoch:    8, Step:   14, loss: 149.74\n",
      "Epoch:    8, Step:   15, loss: 150.07\n",
      "Epoch:    8, Step:   16, loss: 150.40\n",
      "Epoch:    8, Step:   17, loss: 149.63\n",
      "Epoch:    8, Step:   18, loss: 150.68\n",
      "Epoch:    8, Step:   19, loss: 151.37\n",
      "Epoch:    8, Step:   20, loss: 148.54\n",
      "Epoch:    8, Step:   21, loss: 148.70\n",
      "Epoch:    8, Step:   22, loss: 152.53\n",
      "Epoch:    8, Step:   23, loss: 151.53\n",
      "Epoch:    8, Step:   24, loss: 149.77\n",
      "Epoch:    8, Step:   25, loss: 149.66\n",
      "Epoch:    8, Step:   26, loss: 149.38\n",
      "Epoch:    8, Step:   27, loss: 149.41\n",
      "Epoch:    8, Step:   28, loss: 149.77\n",
      "Epoch:    8, Step:   29, loss: 148.75\n",
      "Epoch:    8, Step:   30, loss: 149.34\n",
      "Epoch:    8, Step:   31, loss: 150.49\n",
      "Epoch:    8, Step:   32, loss: 149.90\n",
      "Epoch:    8, Step:   33, loss: 148.87\n",
      "Epoch:    8, Step:   34, loss: 149.71\n",
      "Epoch:    8, Step:   35, loss: 150.03\n",
      "Epoch:    8, Step:   36, loss: 147.61\n",
      "Epoch:    8, Step:   37, loss: 147.83\n",
      "Epoch:    8, Step:   38, loss: 149.66\n",
      "Epoch:    8, Step:   39, loss: 147.03\n",
      "Epoch:    8, Step:   40, loss: 148.95\n",
      "Epoch:    8, Step:   41, loss: 147.22\n",
      "Epoch:    8, Step:   42, loss: 147.58\n",
      "Epoch:    8, Step:   43, loss: 147.54\n",
      "Epoch:    8, Step:   44, loss: 147.86\n",
      "Epoch:    8, Step:   45, loss: 146.67\n",
      "Epoch:    8, Step:   46, loss: 149.23\n",
      "Epoch:    8, Step:   47, loss: 150.03\n",
      "Epoch:    8, Step:   48, loss: 147.73\n",
      "Epoch:    8, Step:   49, loss: 145.46\n",
      "Epoch:    8, Step:   50, loss: 150.17\n",
      "Epoch:    8, Step:   51, loss: 148.58\n",
      "Epoch:    8, Step:   52, loss: 148.62\n",
      "Epoch:    8, Step:   53, loss: 147.49\n",
      "Epoch:    8, Step:   54, loss: 146.25\n",
      "Epoch:    8, Step:   55, loss: 149.81\n",
      "Epoch:    8, Step:   56, loss: 148.34\n",
      "Epoch:    8, Step:   57, loss: 147.48\n",
      "Epoch:    8, Step:   58, loss: 149.14\n",
      "Epoch:    8, Step:   59, loss: 149.38\n",
      "Epoch:    9, Step:    0, loss: 149.10\n",
      "Epoch:    9, Step:    1, loss: 146.47\n",
      "Epoch:    9, Step:    2, loss: 146.47\n",
      "Epoch:    9, Step:    3, loss: 147.06\n",
      "Epoch:    9, Step:    4, loss: 147.09\n",
      "Epoch:    9, Step:    5, loss: 147.69\n",
      "Epoch:    9, Step:    6, loss: 145.77\n",
      "Epoch:    9, Step:    7, loss: 147.24\n",
      "Epoch:    9, Step:    8, loss: 148.02\n",
      "Epoch:    9, Step:    9, loss: 149.14\n",
      "Epoch:    9, Step:   10, loss: 147.63\n",
      "Epoch:    9, Step:   11, loss: 147.38\n",
      "Epoch:    9, Step:   12, loss: 145.78\n",
      "Epoch:    9, Step:   13, loss: 143.88\n",
      "Epoch:    9, Step:   14, loss: 143.60\n",
      "Epoch:    9, Step:   15, loss: 145.81\n",
      "Epoch:    9, Step:   16, loss: 145.74\n",
      "Epoch:    9, Step:   17, loss: 146.82\n",
      "Epoch:    9, Step:   18, loss: 146.27\n",
      "Epoch:    9, Step:   19, loss: 146.33\n",
      "Epoch:    9, Step:   20, loss: 144.93\n",
      "Epoch:    9, Step:   21, loss: 144.90\n",
      "Epoch:    9, Step:   22, loss: 146.01\n",
      "Epoch:    9, Step:   23, loss: 144.78\n",
      "Epoch:    9, Step:   24, loss: 144.59\n",
      "Epoch:    9, Step:   25, loss: 146.68\n",
      "Epoch:    9, Step:   26, loss: 146.11\n",
      "Epoch:    9, Step:   27, loss: 144.62\n",
      "Epoch:    9, Step:   28, loss: 145.19\n",
      "Epoch:    9, Step:   29, loss: 141.50\n",
      "Epoch:    9, Step:   30, loss: 143.75\n",
      "Epoch:    9, Step:   31, loss: 143.37\n",
      "Epoch:    9, Step:   32, loss: 144.30\n",
      "Epoch:    9, Step:   33, loss: 144.44\n",
      "Epoch:    9, Step:   34, loss: 144.07\n",
      "Epoch:    9, Step:   35, loss: 145.30\n",
      "Epoch:    9, Step:   36, loss: 144.03\n",
      "Epoch:    9, Step:   37, loss: 144.48\n",
      "Epoch:    9, Step:   38, loss: 145.80\n",
      "Epoch:    9, Step:   39, loss: 141.04\n",
      "Epoch:    9, Step:   40, loss: 143.20\n",
      "Epoch:    9, Step:   41, loss: 144.03\n",
      "Epoch:    9, Step:   42, loss: 143.97\n",
      "Epoch:    9, Step:   43, loss: 143.14\n",
      "Epoch:    9, Step:   44, loss: 143.78\n",
      "Epoch:    9, Step:   45, loss: 144.14\n",
      "Epoch:    9, Step:   46, loss: 142.99\n",
      "Epoch:    9, Step:   47, loss: 144.54\n",
      "Epoch:    9, Step:   48, loss: 140.48\n",
      "Epoch:    9, Step:   49, loss: 142.39\n",
      "Epoch:    9, Step:   50, loss: 141.90\n",
      "Epoch:    9, Step:   51, loss: 141.61\n",
      "Epoch:    9, Step:   52, loss: 144.10\n",
      "Epoch:    9, Step:   53, loss: 142.40\n",
      "Epoch:    9, Step:   54, loss: 141.81\n",
      "Epoch:    9, Step:   55, loss: 141.80\n",
      "Epoch:    9, Step:   56, loss: 143.14\n",
      "Epoch:    9, Step:   57, loss: 144.40\n",
      "Epoch:    9, Step:   58, loss: 142.44\n",
      "Epoch:    9, Step:   59, loss: 141.08\n",
      "Epoch:   10, Step:    0, loss: 142.54\n",
      "Epoch:   10, Step:    1, loss: 140.62\n",
      "Epoch:   10, Step:    2, loss: 140.36\n",
      "Epoch:   10, Step:    3, loss: 142.40\n",
      "Epoch:   10, Step:    4, loss: 140.96\n",
      "Epoch:   10, Step:    5, loss: 143.04\n",
      "Epoch:   10, Step:    6, loss: 144.63\n",
      "Epoch:   10, Step:    7, loss: 141.48\n",
      "Epoch:   10, Step:    8, loss: 141.03\n",
      "Epoch:   10, Step:    9, loss: 141.51\n",
      "Epoch:   10, Step:   10, loss: 142.53\n",
      "Epoch:   10, Step:   11, loss: 141.41\n",
      "Epoch:   10, Step:   12, loss: 142.33\n",
      "Epoch:   10, Step:   13, loss: 140.49\n",
      "Epoch:   10, Step:   14, loss: 141.46\n",
      "Epoch:   10, Step:   15, loss: 142.06\n",
      "Epoch:   10, Step:   16, loss: 139.65\n",
      "Epoch:   10, Step:   17, loss: 141.34\n",
      "Epoch:   10, Step:   18, loss: 143.04\n",
      "Epoch:   10, Step:   19, loss: 141.78\n",
      "Epoch:   10, Step:   20, loss: 140.31\n",
      "Epoch:   10, Step:   21, loss: 139.84\n",
      "Epoch:   10, Step:   22, loss: 141.65\n",
      "Epoch:   10, Step:   23, loss: 141.14\n",
      "Epoch:   10, Step:   24, loss: 139.53\n",
      "Epoch:   10, Step:   25, loss: 142.16\n",
      "Epoch:   10, Step:   26, loss: 138.90\n",
      "Epoch:   10, Step:   27, loss: 139.78\n",
      "Epoch:   10, Step:   28, loss: 142.37\n",
      "Epoch:   10, Step:   29, loss: 139.80\n",
      "Epoch:   10, Step:   30, loss: 140.00\n",
      "Epoch:   10, Step:   31, loss: 140.57\n",
      "Epoch:   10, Step:   32, loss: 140.92\n",
      "Epoch:   10, Step:   33, loss: 140.98\n",
      "Epoch:   10, Step:   34, loss: 140.39\n",
      "Epoch:   10, Step:   35, loss: 138.29\n",
      "Epoch:   10, Step:   36, loss: 138.87\n",
      "Epoch:   10, Step:   37, loss: 140.52\n",
      "Epoch:   10, Step:   38, loss: 137.93\n",
      "Epoch:   10, Step:   39, loss: 141.39\n",
      "Epoch:   10, Step:   40, loss: 138.97\n",
      "Epoch:   10, Step:   41, loss: 140.31\n",
      "Epoch:   10, Step:   42, loss: 138.79\n",
      "Epoch:   10, Step:   43, loss: 140.10\n",
      "Epoch:   10, Step:   44, loss: 138.13\n",
      "Epoch:   10, Step:   45, loss: 138.26\n",
      "Epoch:   10, Step:   46, loss: 138.61\n",
      "Epoch:   10, Step:   47, loss: 139.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   10, Step:   48, loss: 138.62\n",
      "Epoch:   10, Step:   49, loss: 139.11\n",
      "Epoch:   10, Step:   50, loss: 137.81\n",
      "Epoch:   10, Step:   51, loss: 138.89\n",
      "Epoch:   10, Step:   52, loss: 138.60\n",
      "Epoch:   10, Step:   53, loss: 140.92\n",
      "Epoch:   10, Step:   54, loss: 136.70\n",
      "Epoch:   10, Step:   55, loss: 139.98\n",
      "Epoch:   10, Step:   56, loss: 137.31\n",
      "Epoch:   10, Step:   57, loss: 139.13\n",
      "Epoch:   10, Step:   58, loss: 136.57\n",
      "Epoch:   10, Step:   59, loss: 136.84\n",
      "Epoch:   11, Step:    0, loss: 139.86\n",
      "Epoch:   11, Step:    1, loss: 138.69\n",
      "Epoch:   11, Step:    2, loss: 137.02\n",
      "Epoch:   11, Step:    3, loss: 138.06\n",
      "Epoch:   11, Step:    4, loss: 138.68\n",
      "Epoch:   11, Step:    5, loss: 137.97\n",
      "Epoch:   11, Step:    6, loss: 136.70\n",
      "Epoch:   11, Step:    7, loss: 137.43\n",
      "Epoch:   11, Step:    8, loss: 137.65\n",
      "Epoch:   11, Step:    9, loss: 139.43\n",
      "Epoch:   11, Step:   10, loss: 139.41\n",
      "Epoch:   11, Step:   11, loss: 135.97\n",
      "Epoch:   11, Step:   12, loss: 137.90\n",
      "Epoch:   11, Step:   13, loss: 137.21\n",
      "Epoch:   11, Step:   14, loss: 134.59\n",
      "Epoch:   11, Step:   15, loss: 137.44\n",
      "Epoch:   11, Step:   16, loss: 137.82\n",
      "Epoch:   11, Step:   17, loss: 134.80\n",
      "Epoch:   11, Step:   18, loss: 137.11\n",
      "Epoch:   11, Step:   19, loss: 138.48\n",
      "Epoch:   11, Step:   20, loss: 135.77\n",
      "Epoch:   11, Step:   21, loss: 135.19\n",
      "Epoch:   11, Step:   22, loss: 137.94\n",
      "Epoch:   11, Step:   23, loss: 136.79\n",
      "Epoch:   11, Step:   24, loss: 135.95\n",
      "Epoch:   11, Step:   25, loss: 135.50\n",
      "Epoch:   11, Step:   26, loss: 138.72\n",
      "Epoch:   11, Step:   27, loss: 135.43\n",
      "Epoch:   11, Step:   28, loss: 139.15\n",
      "Epoch:   11, Step:   29, loss: 137.36\n",
      "Epoch:   11, Step:   30, loss: 137.31\n",
      "Epoch:   11, Step:   31, loss: 135.31\n",
      "Epoch:   11, Step:   32, loss: 136.45\n",
      "Epoch:   11, Step:   33, loss: 135.03\n",
      "Epoch:   11, Step:   34, loss: 135.38\n",
      "Epoch:   11, Step:   35, loss: 137.02\n",
      "Epoch:   11, Step:   36, loss: 137.29\n",
      "Epoch:   11, Step:   37, loss: 136.51\n",
      "Epoch:   11, Step:   38, loss: 136.77\n",
      "Epoch:   11, Step:   39, loss: 135.63\n",
      "Epoch:   11, Step:   40, loss: 134.20\n",
      "Epoch:   11, Step:   41, loss: 136.34\n",
      "Epoch:   11, Step:   42, loss: 137.44\n",
      "Epoch:   11, Step:   43, loss: 135.66\n",
      "Epoch:   11, Step:   44, loss: 135.56\n",
      "Epoch:   11, Step:   45, loss: 135.21\n",
      "Epoch:   11, Step:   46, loss: 135.67\n",
      "Epoch:   11, Step:   47, loss: 136.08\n",
      "Epoch:   11, Step:   48, loss: 135.65\n",
      "Epoch:   11, Step:   49, loss: 135.15\n",
      "Epoch:   11, Step:   50, loss: 134.48\n",
      "Epoch:   11, Step:   51, loss: 133.75\n",
      "Epoch:   11, Step:   52, loss: 135.58\n",
      "Epoch:   11, Step:   53, loss: 135.74\n",
      "Epoch:   11, Step:   54, loss: 137.89\n",
      "Epoch:   11, Step:   55, loss: 136.78\n",
      "Epoch:   11, Step:   56, loss: 133.55\n",
      "Epoch:   11, Step:   57, loss: 132.70\n",
      "Epoch:   11, Step:   58, loss: 135.00\n",
      "Epoch:   11, Step:   59, loss: 134.25\n",
      "Epoch:   12, Step:    0, loss: 133.14\n",
      "Epoch:   12, Step:    1, loss: 136.29\n",
      "Epoch:   12, Step:    2, loss: 135.94\n",
      "Epoch:   12, Step:    3, loss: 133.94\n",
      "Epoch:   12, Step:    4, loss: 134.35\n",
      "Epoch:   12, Step:    5, loss: 134.62\n",
      "Epoch:   12, Step:    6, loss: 134.60\n",
      "Epoch:   12, Step:    7, loss: 134.44\n",
      "Epoch:   12, Step:    8, loss: 134.82\n",
      "Epoch:   12, Step:    9, loss: 135.76\n",
      "Epoch:   12, Step:   10, loss: 132.85\n",
      "Epoch:   12, Step:   11, loss: 133.29\n",
      "Epoch:   12, Step:   12, loss: 135.05\n",
      "Epoch:   12, Step:   13, loss: 136.21\n",
      "Epoch:   12, Step:   14, loss: 134.80\n",
      "Epoch:   12, Step:   15, loss: 135.80\n",
      "Epoch:   12, Step:   16, loss: 133.60\n",
      "Epoch:   12, Step:   17, loss: 134.54\n",
      "Epoch:   12, Step:   18, loss: 132.57\n",
      "Epoch:   12, Step:   19, loss: 135.30\n",
      "Epoch:   12, Step:   20, loss: 135.95\n",
      "Epoch:   12, Step:   21, loss: 132.70\n",
      "Epoch:   12, Step:   22, loss: 134.41\n",
      "Epoch:   12, Step:   23, loss: 134.49\n",
      "Epoch:   12, Step:   24, loss: 135.02\n",
      "Epoch:   12, Step:   25, loss: 134.37\n",
      "Epoch:   12, Step:   26, loss: 131.98\n",
      "Epoch:   12, Step:   27, loss: 132.56\n",
      "Epoch:   12, Step:   28, loss: 134.79\n",
      "Epoch:   12, Step:   29, loss: 132.81\n",
      "Epoch:   12, Step:   30, loss: 134.90\n",
      "Epoch:   12, Step:   31, loss: 131.84\n",
      "Epoch:   12, Step:   32, loss: 133.55\n",
      "Epoch:   12, Step:   33, loss: 133.44\n",
      "Epoch:   12, Step:   34, loss: 133.14\n",
      "Epoch:   12, Step:   35, loss: 133.26\n",
      "Epoch:   12, Step:   36, loss: 132.59\n",
      "Epoch:   12, Step:   37, loss: 135.21\n",
      "Epoch:   12, Step:   38, loss: 132.12\n",
      "Epoch:   12, Step:   39, loss: 133.51\n",
      "Epoch:   12, Step:   40, loss: 132.97\n",
      "Epoch:   12, Step:   41, loss: 131.22\n",
      "Epoch:   12, Step:   42, loss: 130.79\n",
      "Epoch:   12, Step:   43, loss: 134.28\n",
      "Epoch:   12, Step:   44, loss: 132.12\n",
      "Epoch:   12, Step:   45, loss: 132.02\n",
      "Epoch:   12, Step:   46, loss: 131.46\n",
      "Epoch:   12, Step:   47, loss: 131.69\n",
      "Epoch:   12, Step:   48, loss: 129.36\n",
      "Epoch:   12, Step:   49, loss: 133.90\n",
      "Epoch:   12, Step:   50, loss: 133.38\n",
      "Epoch:   12, Step:   51, loss: 133.72\n",
      "Epoch:   12, Step:   52, loss: 131.88\n",
      "Epoch:   12, Step:   53, loss: 133.26\n",
      "Epoch:   12, Step:   54, loss: 131.70\n",
      "Epoch:   12, Step:   55, loss: 128.75\n",
      "Epoch:   12, Step:   56, loss: 135.14\n",
      "Epoch:   12, Step:   57, loss: 133.13\n",
      "Epoch:   12, Step:   58, loss: 134.95\n",
      "Epoch:   12, Step:   59, loss: 131.75\n",
      "Epoch:   13, Step:    0, loss: 131.51\n",
      "Epoch:   13, Step:    1, loss: 131.90\n",
      "Epoch:   13, Step:    2, loss: 130.32\n",
      "Epoch:   13, Step:    3, loss: 131.98\n",
      "Epoch:   13, Step:    4, loss: 130.46\n",
      "Epoch:   13, Step:    5, loss: 132.75\n",
      "Epoch:   13, Step:    6, loss: 130.46\n",
      "Epoch:   13, Step:    7, loss: 132.18\n",
      "Epoch:   13, Step:    8, loss: 130.21\n",
      "Epoch:   13, Step:    9, loss: 134.49\n",
      "Epoch:   13, Step:   10, loss: 130.30\n",
      "Epoch:   13, Step:   11, loss: 132.72\n",
      "Epoch:   13, Step:   12, loss: 131.09\n",
      "Epoch:   13, Step:   13, loss: 131.80\n",
      "Epoch:   13, Step:   14, loss: 131.97\n",
      "Epoch:   13, Step:   15, loss: 130.86\n",
      "Epoch:   13, Step:   16, loss: 130.83\n",
      "Epoch:   13, Step:   17, loss: 129.28\n",
      "Epoch:   13, Step:   18, loss: 129.21\n",
      "Epoch:   13, Step:   19, loss: 130.56\n",
      "Epoch:   13, Step:   20, loss: 131.78\n",
      "Epoch:   13, Step:   21, loss: 130.61\n",
      "Epoch:   13, Step:   22, loss: 131.49\n",
      "Epoch:   13, Step:   23, loss: 131.97\n",
      "Epoch:   13, Step:   24, loss: 130.86\n",
      "Epoch:   13, Step:   25, loss: 132.42\n",
      "Epoch:   13, Step:   26, loss: 133.11\n",
      "Epoch:   13, Step:   27, loss: 129.79\n",
      "Epoch:   13, Step:   28, loss: 130.41\n",
      "Epoch:   13, Step:   29, loss: 131.88\n",
      "Epoch:   13, Step:   30, loss: 134.30\n",
      "Epoch:   13, Step:   31, loss: 132.06\n",
      "Epoch:   13, Step:   32, loss: 130.31\n",
      "Epoch:   13, Step:   33, loss: 130.22\n",
      "Epoch:   13, Step:   34, loss: 130.95\n",
      "Epoch:   13, Step:   35, loss: 131.48\n",
      "Epoch:   13, Step:   36, loss: 131.20\n",
      "Epoch:   13, Step:   37, loss: 130.92\n",
      "Epoch:   13, Step:   38, loss: 131.75\n",
      "Epoch:   13, Step:   39, loss: 131.24\n",
      "Epoch:   13, Step:   40, loss: 131.17\n",
      "Epoch:   13, Step:   41, loss: 128.71\n",
      "Epoch:   13, Step:   42, loss: 129.43\n",
      "Epoch:   13, Step:   43, loss: 129.62\n",
      "Epoch:   13, Step:   44, loss: 133.36\n",
      "Epoch:   13, Step:   45, loss: 130.93\n",
      "Epoch:   13, Step:   46, loss: 130.05\n",
      "Epoch:   13, Step:   47, loss: 130.50\n",
      "Epoch:   13, Step:   48, loss: 130.79\n",
      "Epoch:   13, Step:   49, loss: 128.71\n",
      "Epoch:   13, Step:   50, loss: 130.52\n",
      "Epoch:   13, Step:   51, loss: 130.30\n",
      "Epoch:   13, Step:   52, loss: 129.28\n",
      "Epoch:   13, Step:   53, loss: 131.29\n",
      "Epoch:   13, Step:   54, loss: 130.43\n",
      "Epoch:   13, Step:   55, loss: 129.53\n",
      "Epoch:   13, Step:   56, loss: 131.41\n",
      "Epoch:   13, Step:   57, loss: 130.43\n",
      "Epoch:   13, Step:   58, loss: 129.69\n",
      "Epoch:   13, Step:   59, loss: 130.33\n",
      "Epoch:   14, Step:    0, loss: 129.18\n",
      "Epoch:   14, Step:    1, loss: 131.33\n",
      "Epoch:   14, Step:    2, loss: 131.48\n",
      "Epoch:   14, Step:    3, loss: 130.16\n",
      "Epoch:   14, Step:    4, loss: 129.78\n",
      "Epoch:   14, Step:    5, loss: 128.43\n",
      "Epoch:   14, Step:    6, loss: 127.97\n",
      "Epoch:   14, Step:    7, loss: 131.81\n",
      "Epoch:   14, Step:    8, loss: 129.68\n",
      "Epoch:   14, Step:    9, loss: 128.05\n",
      "Epoch:   14, Step:   10, loss: 128.53\n",
      "Epoch:   14, Step:   11, loss: 130.14\n",
      "Epoch:   14, Step:   12, loss: 127.91\n",
      "Epoch:   14, Step:   13, loss: 128.59\n",
      "Epoch:   14, Step:   14, loss: 128.98\n",
      "Epoch:   14, Step:   15, loss: 130.61\n",
      "Epoch:   14, Step:   16, loss: 130.08\n",
      "Epoch:   14, Step:   17, loss: 131.10\n",
      "Epoch:   14, Step:   18, loss: 125.84\n",
      "Epoch:   14, Step:   19, loss: 129.99\n",
      "Epoch:   14, Step:   20, loss: 129.47\n",
      "Epoch:   14, Step:   21, loss: 127.44\n",
      "Epoch:   14, Step:   22, loss: 129.50\n",
      "Epoch:   14, Step:   23, loss: 129.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   14, Step:   24, loss: 132.15\n",
      "Epoch:   14, Step:   25, loss: 129.09\n",
      "Epoch:   14, Step:   26, loss: 130.48\n",
      "Epoch:   14, Step:   27, loss: 127.83\n",
      "Epoch:   14, Step:   28, loss: 128.31\n",
      "Epoch:   14, Step:   29, loss: 128.46\n",
      "Epoch:   14, Step:   30, loss: 128.48\n",
      "Epoch:   14, Step:   31, loss: 130.50\n",
      "Epoch:   14, Step:   32, loss: 128.49\n",
      "Epoch:   14, Step:   33, loss: 128.17\n",
      "Epoch:   14, Step:   34, loss: 129.40\n",
      "Epoch:   14, Step:   35, loss: 127.99\n",
      "Epoch:   14, Step:   36, loss: 128.01\n",
      "Epoch:   14, Step:   37, loss: 130.19\n",
      "Epoch:   14, Step:   38, loss: 126.51\n",
      "Epoch:   14, Step:   39, loss: 127.59\n",
      "Epoch:   14, Step:   40, loss: 128.47\n",
      "Epoch:   14, Step:   41, loss: 128.18\n",
      "Epoch:   14, Step:   42, loss: 130.54\n",
      "Epoch:   14, Step:   43, loss: 126.99\n",
      "Epoch:   14, Step:   44, loss: 128.97\n",
      "Epoch:   14, Step:   45, loss: 126.70\n",
      "Epoch:   14, Step:   46, loss: 126.50\n",
      "Epoch:   14, Step:   47, loss: 125.11\n",
      "Epoch:   14, Step:   48, loss: 128.05\n",
      "Epoch:   14, Step:   49, loss: 128.25\n",
      "Epoch:   14, Step:   50, loss: 129.29\n",
      "Epoch:   14, Step:   51, loss: 130.72\n",
      "Epoch:   14, Step:   52, loss: 127.36\n",
      "Epoch:   14, Step:   53, loss: 127.41\n",
      "Epoch:   14, Step:   54, loss: 129.83\n",
      "Epoch:   14, Step:   55, loss: 127.38\n",
      "Epoch:   14, Step:   56, loss: 126.01\n",
      "Epoch:   14, Step:   57, loss: 130.01\n",
      "Epoch:   14, Step:   58, loss: 127.66\n",
      "Epoch:   14, Step:   59, loss: 127.68\n",
      "Epoch:   15, Step:    0, loss: 127.04\n",
      "Epoch:   15, Step:    1, loss: 126.30\n",
      "Epoch:   15, Step:    2, loss: 128.49\n",
      "Epoch:   15, Step:    3, loss: 129.13\n",
      "Epoch:   15, Step:    4, loss: 127.49\n",
      "Epoch:   15, Step:    5, loss: 127.28\n",
      "Epoch:   15, Step:    6, loss: 128.67\n",
      "Epoch:   15, Step:    7, loss: 128.52\n",
      "Epoch:   15, Step:    8, loss: 128.73\n",
      "Epoch:   15, Step:    9, loss: 125.86\n",
      "Epoch:   15, Step:   10, loss: 127.56\n",
      "Epoch:   15, Step:   11, loss: 125.86\n",
      "Epoch:   15, Step:   12, loss: 127.07\n",
      "Epoch:   15, Step:   13, loss: 128.30\n",
      "Epoch:   15, Step:   14, loss: 126.78\n",
      "Epoch:   15, Step:   15, loss: 127.07\n",
      "Epoch:   15, Step:   16, loss: 128.67\n",
      "Epoch:   15, Step:   17, loss: 127.57\n",
      "Epoch:   15, Step:   18, loss: 126.95\n",
      "Epoch:   15, Step:   19, loss: 127.48\n",
      "Epoch:   15, Step:   20, loss: 127.47\n",
      "Epoch:   15, Step:   21, loss: 126.34\n",
      "Epoch:   15, Step:   22, loss: 126.34\n",
      "Epoch:   15, Step:   23, loss: 126.49\n",
      "Epoch:   15, Step:   24, loss: 128.76\n",
      "Epoch:   15, Step:   25, loss: 126.65\n",
      "Epoch:   15, Step:   26, loss: 127.63\n",
      "Epoch:   15, Step:   27, loss: 125.31\n",
      "Epoch:   15, Step:   28, loss: 125.12\n",
      "Epoch:   15, Step:   29, loss: 126.13\n",
      "Epoch:   15, Step:   30, loss: 127.22\n",
      "Epoch:   15, Step:   31, loss: 127.21\n",
      "Epoch:   15, Step:   32, loss: 129.59\n",
      "Epoch:   15, Step:   33, loss: 124.87\n",
      "Epoch:   15, Step:   34, loss: 125.98\n",
      "Epoch:   15, Step:   35, loss: 126.74\n",
      "Epoch:   15, Step:   36, loss: 126.75\n",
      "Epoch:   15, Step:   37, loss: 127.89\n",
      "Epoch:   15, Step:   38, loss: 126.63\n",
      "Epoch:   15, Step:   39, loss: 125.64\n",
      "Epoch:   15, Step:   40, loss: 126.81\n",
      "Epoch:   15, Step:   41, loss: 124.53\n",
      "Epoch:   15, Step:   42, loss: 125.90\n",
      "Epoch:   15, Step:   43, loss: 126.46\n",
      "Epoch:   15, Step:   44, loss: 125.39\n",
      "Epoch:   15, Step:   45, loss: 127.44\n",
      "Epoch:   15, Step:   46, loss: 129.16\n",
      "Epoch:   15, Step:   47, loss: 127.22\n",
      "Epoch:   15, Step:   48, loss: 127.43\n",
      "Epoch:   15, Step:   49, loss: 126.37\n",
      "Epoch:   15, Step:   50, loss: 126.16\n",
      "Epoch:   15, Step:   51, loss: 125.55\n",
      "Epoch:   15, Step:   52, loss: 124.76\n",
      "Epoch:   15, Step:   53, loss: 124.00\n",
      "Epoch:   15, Step:   54, loss: 125.23\n",
      "Epoch:   15, Step:   55, loss: 127.40\n",
      "Epoch:   15, Step:   56, loss: 128.43\n",
      "Epoch:   15, Step:   57, loss: 125.40\n",
      "Epoch:   15, Step:   58, loss: 125.17\n",
      "Epoch:   15, Step:   59, loss: 128.15\n",
      "Epoch:   16, Step:    0, loss: 125.14\n",
      "Epoch:   16, Step:    1, loss: 125.55\n",
      "Epoch:   16, Step:    2, loss: 124.04\n",
      "Epoch:   16, Step:    3, loss: 125.30\n",
      "Epoch:   16, Step:    4, loss: 126.51\n",
      "Epoch:   16, Step:    5, loss: 127.42\n",
      "Epoch:   16, Step:    6, loss: 124.93\n",
      "Epoch:   16, Step:    7, loss: 125.73\n",
      "Epoch:   16, Step:    8, loss: 124.47\n",
      "Epoch:   16, Step:    9, loss: 126.05\n",
      "Epoch:   16, Step:   10, loss: 127.32\n",
      "Epoch:   16, Step:   11, loss: 124.54\n",
      "Epoch:   16, Step:   12, loss: 125.74\n",
      "Epoch:   16, Step:   13, loss: 123.52\n",
      "Epoch:   16, Step:   14, loss: 125.00\n",
      "Epoch:   16, Step:   15, loss: 125.49\n",
      "Epoch:   16, Step:   16, loss: 126.57\n",
      "Epoch:   16, Step:   17, loss: 125.40\n",
      "Epoch:   16, Step:   18, loss: 126.61\n",
      "Epoch:   16, Step:   19, loss: 127.42\n",
      "Epoch:   16, Step:   20, loss: 125.25\n",
      "Epoch:   16, Step:   21, loss: 124.93\n",
      "Epoch:   16, Step:   22, loss: 124.89\n",
      "Epoch:   16, Step:   23, loss: 125.06\n",
      "Epoch:   16, Step:   24, loss: 127.72\n",
      "Epoch:   16, Step:   25, loss: 125.30\n",
      "Epoch:   16, Step:   26, loss: 123.69\n",
      "Epoch:   16, Step:   27, loss: 123.02\n",
      "Epoch:   16, Step:   28, loss: 125.50\n",
      "Epoch:   16, Step:   29, loss: 125.06\n",
      "Epoch:   16, Step:   30, loss: 123.94\n",
      "Epoch:   16, Step:   31, loss: 124.76\n",
      "Epoch:   16, Step:   32, loss: 125.89\n",
      "Epoch:   16, Step:   33, loss: 124.10\n",
      "Epoch:   16, Step:   34, loss: 123.96\n",
      "Epoch:   16, Step:   35, loss: 123.03\n",
      "Epoch:   16, Step:   36, loss: 124.40\n",
      "Epoch:   16, Step:   37, loss: 126.96\n",
      "Epoch:   16, Step:   38, loss: 123.26\n",
      "Epoch:   16, Step:   39, loss: 126.05\n",
      "Epoch:   16, Step:   40, loss: 125.54\n",
      "Epoch:   16, Step:   41, loss: 123.15\n",
      "Epoch:   16, Step:   42, loss: 125.60\n",
      "Epoch:   16, Step:   43, loss: 124.89\n",
      "Epoch:   16, Step:   44, loss: 123.88\n",
      "Epoch:   16, Step:   45, loss: 125.40\n",
      "Epoch:   16, Step:   46, loss: 124.48\n",
      "Epoch:   16, Step:   47, loss: 124.57\n",
      "Epoch:   16, Step:   48, loss: 124.87\n",
      "Epoch:   16, Step:   49, loss: 124.45\n",
      "Epoch:   16, Step:   50, loss: 126.09\n",
      "Epoch:   16, Step:   51, loss: 121.91\n",
      "Epoch:   16, Step:   52, loss: 124.65\n",
      "Epoch:   16, Step:   53, loss: 125.61\n",
      "Epoch:   16, Step:   54, loss: 124.05\n",
      "Epoch:   16, Step:   55, loss: 123.57\n",
      "Epoch:   16, Step:   56, loss: 124.54\n",
      "Epoch:   16, Step:   57, loss: 123.53\n",
      "Epoch:   16, Step:   58, loss: 122.96\n",
      "Epoch:   16, Step:   59, loss: 126.69\n",
      "Epoch:   17, Step:    0, loss: 122.34\n",
      "Epoch:   17, Step:    1, loss: 123.46\n",
      "Epoch:   17, Step:    2, loss: 124.08\n",
      "Epoch:   17, Step:    3, loss: 122.85\n",
      "Epoch:   17, Step:    4, loss: 124.10\n",
      "Epoch:   17, Step:    5, loss: 123.17\n",
      "Epoch:   17, Step:    6, loss: 125.02\n",
      "Epoch:   17, Step:    7, loss: 122.91\n",
      "Epoch:   17, Step:    8, loss: 123.22\n",
      "Epoch:   17, Step:    9, loss: 123.39\n",
      "Epoch:   17, Step:   10, loss: 123.14\n",
      "Epoch:   17, Step:   11, loss: 125.55\n",
      "Epoch:   17, Step:   12, loss: 123.45\n",
      "Epoch:   17, Step:   13, loss: 125.23\n",
      "Epoch:   17, Step:   14, loss: 121.13\n",
      "Epoch:   17, Step:   15, loss: 124.06\n",
      "Epoch:   17, Step:   16, loss: 123.13\n",
      "Epoch:   17, Step:   17, loss: 122.44\n",
      "Epoch:   17, Step:   18, loss: 124.50\n",
      "Epoch:   17, Step:   19, loss: 123.07\n",
      "Epoch:   17, Step:   20, loss: 124.80\n",
      "Epoch:   17, Step:   21, loss: 123.63\n",
      "Epoch:   17, Step:   22, loss: 124.83\n",
      "Epoch:   17, Step:   23, loss: 125.02\n",
      "Epoch:   17, Step:   24, loss: 122.45\n",
      "Epoch:   17, Step:   25, loss: 122.02\n",
      "Epoch:   17, Step:   26, loss: 123.96\n",
      "Epoch:   17, Step:   27, loss: 123.25\n",
      "Epoch:   17, Step:   28, loss: 123.68\n",
      "Epoch:   17, Step:   29, loss: 123.14\n",
      "Epoch:   17, Step:   30, loss: 124.08\n",
      "Epoch:   17, Step:   31, loss: 124.18\n",
      "Epoch:   17, Step:   32, loss: 121.70\n",
      "Epoch:   17, Step:   33, loss: 123.86\n",
      "Epoch:   17, Step:   34, loss: 123.08\n",
      "Epoch:   17, Step:   35, loss: 123.29\n",
      "Epoch:   17, Step:   36, loss: 123.09\n",
      "Epoch:   17, Step:   37, loss: 123.40\n",
      "Epoch:   17, Step:   38, loss: 123.90\n",
      "Epoch:   17, Step:   39, loss: 122.93\n",
      "Epoch:   17, Step:   40, loss: 124.91\n",
      "Epoch:   17, Step:   41, loss: 121.46\n",
      "Epoch:   17, Step:   42, loss: 122.57\n",
      "Epoch:   17, Step:   43, loss: 125.76\n",
      "Epoch:   17, Step:   44, loss: 123.86\n",
      "Epoch:   17, Step:   45, loss: 122.61\n",
      "Epoch:   17, Step:   46, loss: 121.46\n",
      "Epoch:   17, Step:   47, loss: 122.29\n",
      "Epoch:   17, Step:   48, loss: 123.01\n",
      "Epoch:   17, Step:   49, loss: 121.83\n",
      "Epoch:   17, Step:   50, loss: 123.78\n",
      "Epoch:   17, Step:   51, loss: 124.03\n",
      "Epoch:   17, Step:   52, loss: 120.70\n",
      "Epoch:   17, Step:   53, loss: 123.09\n",
      "Epoch:   17, Step:   54, loss: 121.79\n",
      "Epoch:   17, Step:   55, loss: 124.35\n",
      "Epoch:   17, Step:   56, loss: 123.22\n",
      "Epoch:   17, Step:   57, loss: 123.92\n",
      "Epoch:   17, Step:   58, loss: 121.62\n",
      "Epoch:   17, Step:   59, loss: 123.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   18, Step:    0, loss: 121.47\n",
      "Epoch:   18, Step:    1, loss: 122.59\n",
      "Epoch:   18, Step:    2, loss: 122.96\n",
      "Epoch:   18, Step:    3, loss: 121.66\n",
      "Epoch:   18, Step:    4, loss: 124.52\n",
      "Epoch:   18, Step:    5, loss: 123.49\n",
      "Epoch:   18, Step:    6, loss: 121.57\n",
      "Epoch:   18, Step:    7, loss: 120.47\n",
      "Epoch:   18, Step:    8, loss: 121.92\n",
      "Epoch:   18, Step:    9, loss: 124.09\n",
      "Epoch:   18, Step:   10, loss: 120.13\n",
      "Epoch:   18, Step:   11, loss: 122.17\n",
      "Epoch:   18, Step:   12, loss: 120.82\n",
      "Epoch:   18, Step:   13, loss: 120.57\n",
      "Epoch:   18, Step:   14, loss: 125.66\n",
      "Epoch:   18, Step:   15, loss: 120.40\n",
      "Epoch:   18, Step:   16, loss: 123.99\n",
      "Epoch:   18, Step:   17, loss: 121.95\n",
      "Epoch:   18, Step:   18, loss: 124.02\n",
      "Epoch:   18, Step:   19, loss: 122.71\n",
      "Epoch:   18, Step:   20, loss: 122.12\n",
      "Epoch:   18, Step:   21, loss: 121.91\n",
      "Epoch:   18, Step:   22, loss: 123.81\n",
      "Epoch:   18, Step:   23, loss: 121.09\n",
      "Epoch:   18, Step:   24, loss: 121.05\n",
      "Epoch:   18, Step:   25, loss: 120.18\n",
      "Epoch:   18, Step:   26, loss: 122.38\n",
      "Epoch:   18, Step:   27, loss: 122.25\n",
      "Epoch:   18, Step:   28, loss: 122.01\n",
      "Epoch:   18, Step:   29, loss: 121.44\n",
      "Epoch:   18, Step:   30, loss: 122.58\n",
      "Epoch:   18, Step:   31, loss: 124.60\n",
      "Epoch:   18, Step:   32, loss: 121.28\n",
      "Epoch:   18, Step:   33, loss: 122.18\n",
      "Epoch:   18, Step:   34, loss: 120.71\n",
      "Epoch:   18, Step:   35, loss: 121.38\n",
      "Epoch:   18, Step:   36, loss: 120.71\n",
      "Epoch:   18, Step:   37, loss: 122.50\n",
      "Epoch:   18, Step:   38, loss: 122.43\n",
      "Epoch:   18, Step:   39, loss: 121.85\n",
      "Epoch:   18, Step:   40, loss: 121.17\n",
      "Epoch:   18, Step:   41, loss: 119.71\n",
      "Epoch:   18, Step:   42, loss: 122.12\n",
      "Epoch:   18, Step:   43, loss: 121.30\n",
      "Epoch:   18, Step:   44, loss: 122.90\n",
      "Epoch:   18, Step:   45, loss: 120.62\n",
      "Epoch:   18, Step:   46, loss: 120.36\n",
      "Epoch:   18, Step:   47, loss: 122.11\n",
      "Epoch:   18, Step:   48, loss: 122.96\n",
      "Epoch:   18, Step:   49, loss: 122.42\n",
      "Epoch:   18, Step:   50, loss: 122.46\n",
      "Epoch:   18, Step:   51, loss: 121.95\n",
      "Epoch:   18, Step:   52, loss: 119.25\n",
      "Epoch:   18, Step:   53, loss: 121.39\n",
      "Epoch:   18, Step:   54, loss: 120.59\n",
      "Epoch:   18, Step:   55, loss: 121.23\n",
      "Epoch:   18, Step:   56, loss: 119.40\n",
      "Epoch:   18, Step:   57, loss: 121.86\n",
      "Epoch:   18, Step:   58, loss: 120.79\n",
      "Epoch:   18, Step:   59, loss: 120.39\n",
      "Epoch:   19, Step:    0, loss: 119.85\n",
      "Epoch:   19, Step:    1, loss: 119.76\n",
      "Epoch:   19, Step:    2, loss: 122.80\n",
      "Epoch:   19, Step:    3, loss: 120.94\n",
      "Epoch:   19, Step:    4, loss: 120.91\n",
      "Epoch:   19, Step:    5, loss: 121.91\n",
      "Epoch:   19, Step:    6, loss: 120.07\n",
      "Epoch:   19, Step:    7, loss: 121.34\n",
      "Epoch:   19, Step:    8, loss: 121.42\n",
      "Epoch:   19, Step:    9, loss: 121.05\n",
      "Epoch:   19, Step:   10, loss: 120.58\n",
      "Epoch:   19, Step:   11, loss: 121.35\n",
      "Epoch:   19, Step:   12, loss: 120.09\n",
      "Epoch:   19, Step:   13, loss: 120.18\n",
      "Epoch:   19, Step:   14, loss: 120.37\n",
      "Epoch:   19, Step:   15, loss: 122.35\n",
      "Epoch:   19, Step:   16, loss: 121.47\n",
      "Epoch:   19, Step:   17, loss: 120.96\n",
      "Epoch:   19, Step:   18, loss: 120.63\n",
      "Epoch:   19, Step:   19, loss: 120.47\n",
      "Epoch:   19, Step:   20, loss: 120.86\n",
      "Epoch:   19, Step:   21, loss: 121.54\n",
      "Epoch:   19, Step:   22, loss: 120.44\n",
      "Epoch:   19, Step:   23, loss: 121.16\n",
      "Epoch:   19, Step:   24, loss: 120.36\n",
      "Epoch:   19, Step:   25, loss: 119.64\n",
      "Epoch:   19, Step:   26, loss: 122.20\n",
      "Epoch:   19, Step:   27, loss: 119.52\n",
      "Epoch:   19, Step:   28, loss: 120.40\n",
      "Epoch:   19, Step:   29, loss: 121.48\n",
      "Epoch:   19, Step:   30, loss: 121.10\n",
      "Epoch:   19, Step:   31, loss: 120.22\n",
      "Epoch:   19, Step:   32, loss: 121.16\n",
      "Epoch:   19, Step:   33, loss: 119.17\n",
      "Epoch:   19, Step:   34, loss: 120.82\n",
      "Epoch:   19, Step:   35, loss: 119.65\n",
      "Epoch:   19, Step:   36, loss: 121.49\n",
      "Epoch:   19, Step:   37, loss: 120.16\n",
      "Epoch:   19, Step:   38, loss: 120.58\n",
      "Epoch:   19, Step:   39, loss: 119.68\n",
      "Epoch:   19, Step:   40, loss: 119.60\n",
      "Epoch:   19, Step:   41, loss: 119.37\n",
      "Epoch:   19, Step:   42, loss: 119.20\n",
      "Epoch:   19, Step:   43, loss: 121.05\n",
      "Epoch:   19, Step:   44, loss: 121.01\n",
      "Epoch:   19, Step:   45, loss: 120.00\n",
      "Epoch:   19, Step:   46, loss: 118.07\n",
      "Epoch:   19, Step:   47, loss: 119.92\n",
      "Epoch:   19, Step:   48, loss: 117.61\n",
      "Epoch:   19, Step:   49, loss: 118.85\n",
      "Epoch:   19, Step:   50, loss: 119.44\n",
      "Epoch:   19, Step:   51, loss: 120.76\n",
      "Epoch:   19, Step:   52, loss: 119.95\n",
      "Epoch:   19, Step:   53, loss: 120.86\n",
      "Epoch:   19, Step:   54, loss: 118.99\n",
      "Epoch:   19, Step:   55, loss: 121.18\n",
      "Epoch:   19, Step:   56, loss: 119.55\n",
      "Epoch:   19, Step:   57, loss: 116.94\n",
      "Epoch:   19, Step:   58, loss: 121.07\n",
      "Epoch:   19, Step:   59, loss: 121.29\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "num_epoches = 20\n",
    "train_loss_epoch = []\n",
    "for epoch in range(num_epoches):\n",
    "    running_loss = [] \n",
    "    for idx, data in enumerate(train_data_loader):\n",
    "        inputs = Variable(data[0])\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        inputs = inputs.expand(5, 1000, 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = vae.train_loss(inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        print((\"Epoch: {:>4}, Step: {:>4}, loss: {:>4.2f}\")\n",
    "              .format(epoch, idx, loss.item()), flush = True)\n",
    "        running_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_chosen = 414\n",
    "\n",
    "image_og = (inputs[0][number_chosen])\n",
    "img = image_og.view(28, 28).data\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "image = (vae.forward(inputs[0][number_chosen])[1]) \n",
    "img = image.view(28, 28).data\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
